{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "JkIYFfDPqryq",
        "outputId": "88f5942a-d9fc-4f43-fe15-210e1b12a3f2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 31)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    (x.train,_),(_, _) =tf.keras.datasets.mnist.load_data()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "#import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#define generator model\n",
        "def built_generator(latent_dim):\n",
        "  model=tf.keras.Sequencial()\n",
        "  model.add(layers.Dense(128,input_dim=latent_dim,activation='relu'))\n",
        "  model.add(layers.Dense(784,activation='sigmoid'))\n",
        "  model.add(layers.Reshape(28,28,1))\n",
        "  return model\n",
        "#define discrimator\n",
        "def built_discriminator(img_shape):\n",
        "  model=tf.keras.Sequential()\n",
        "  model.add(layers.flattern(input_shapping_shape))\n",
        "  model.add(layers.Dense(128,activation='relu'))\n",
        "  model.add(layers.Dense(1,activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "  #define the gan model by combining generator and descriminator\n",
        "def built_gan(generator,discriminator):\n",
        "   discriminator.trainable=false\n",
        "   model=tf.keras.Sequential()\n",
        "   model.add(generator)\n",
        "   model.add(discriminator)\n",
        "   return model\n",
        "\n",
        "  #load MNIDT Dataset\n",
        "  (x.train,_),(_, _) =tf.keras.datasets.mnist.load_data()\n",
        "  x_train = x_train/ 255.0\n",
        "\n",
        "  #Reshape and normalize images\n",
        "  x_train = x_train.reshape((x_train.shape[0],28,28,1))\n",
        "\n",
        "  #built and compile the discriminator\n",
        "discriminator = build_discriminator(28,28,1)\n",
        "discriminator.compile(optimizer='adam',loss='binary_crossentropy',metrices['accuracy'])\n",
        "\n",
        "#built and compile the generator\n",
        "latent_dim = 100\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "#built and compile the gan model\n",
        "discriminator.trainable = False\n",
        "gan = build_gan(generator,discriminator)\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "#Training the gan\n",
        "epochs = 10000\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #generate random noise as input to the generator\n",
        "  noise = np.random.normal(0,1, size=(batch_size, latent_dim))\n",
        "\n",
        "  #generate fake images using the generator\n",
        "  generated_images = generator.predict(noise)\n",
        "\n",
        "  #select a random batch of real image and real images\n",
        "  idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "  real_images = x_train[idx]\n",
        "\n",
        "  label_real = np.ones((batch_size,1))\n",
        "  label_fake = np.zeros((batch_size,1))\n",
        "\n",
        "  d_loss_real = discriminator.train_on_batch(real_image, labels_real)\n",
        "  d_loss_fake = discriminator.train_on_batch(generated_image, labels_fake)\n",
        "\n",
        "  d_loss = 0.5 + np.add(d_loss_real,d_loss_fake)\n",
        "\n",
        "  noise=np.random.normal(0,1, size=(batch_size, latent_dim))\n",
        "  labels_gan = np.ones((batch_size, 1))\n",
        "  g_loss = gan.train_on_batch(noise, labels_gan)\n",
        "\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch {epoch}, Discriminator Loss: {d_loss[0]},Generator Loss:{g_loss}\")\n",
        "    gen_imgs = generator.predict(np.random.normal(0,1, size=(16, latent_dim)))\n",
        "    gen_imgs = 0.5 + gen_imgs + 0.5\n",
        "    fig, axs = plt.subplots(4,4)\n",
        "    count = 0\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "          axs[i,j].imshow(gen_imgs[count, :, :, 0], cmap='gray')\n",
        "          axs[i,j].axis('off')\n",
        "          count += 1\n",
        "plt.show()"
      ]
    }
  ]
}